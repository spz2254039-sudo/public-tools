# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f6aNUt_h51FxlZgUBEEVpg8X4LjnvueY
"""

# === å®‰è£å¿…è¦å¥—ä»¶ ===
#!pip install requests beautifulsoup4 pandas python-docx

import re
import time
import html
import requests
from bs4 import BeautifulSoup
from docx import Document
from docx.shared import Pt, Cm, RGBColor

# â¤ å° MOMO å•†å“é€²è¡Œ retry + timeout çš„ç°¡æ˜“çˆ¬èŸ²
def parse_momo_simple(url, max_retries=3):
    headers = {"User-Agent": "Mozilla/5.0"}
    for attempt in range(1, max_retries + 1):
        try:
            res = requests.get(url, headers=headers, timeout=10)
            res.raise_for_status()
            soup = BeautifulSoup(res.text, "html.parser")

            # å•†å“åç¨±
            name = soup.select_one("meta[property='og:title']")
            name = name["content"].strip() if name else "æœªå–å¾—"

            # å“è™Ÿï¼ˆå¤šé‡å˜—è©¦ï¼‰
            prod_no = None
            tag = soup.select_one("#osmPrdNo")
            if tag: prod_no = tag.text.strip()
            if not prod_no:
                li_tags = soup.select("li.tvlogo, li.goods-code-container")
                for tag in li_tags:
                    if "å“è™Ÿï¼š" in tag.text:
                        prod_no = tag.text.split("å“è™Ÿï¼š")[-1].strip()
                        break
            if not prod_no:
                meta_code = soup.select_one("meta[name='keywords']")
                if meta_code and "å“è™Ÿï¼š" in meta_code["content"]:
                    prod_no = meta_code["content"].split("å“è™Ÿï¼š")[-1].split(",")[0].strip()
            if not prod_no:
                match = re.search(r"å“è™Ÿ[:ï¼š ]?\s*(\w+)", soup.get_text())
                prod_no = match.group(1).strip() if match else "æœªå–å¾—"

            return {
                "å•†å“åç¨±": name,
                "å“è™Ÿ": prod_no,
                "ç¶²å€": url
            }

        except Exception as e:
            print(f"âŒ ç¬¬ {attempt} æ¬¡å¤±æ•—ï¼š{e}")
            if attempt < max_retries:
                time.sleep(3)
            else:
                return {
                    "å•†å“åç¨±": f"éŒ¯èª¤ï¼š{e}",
                    "å“è™Ÿ": "éŒ¯èª¤",
                    "ç¶²å€": url
                }

# â¤ æ“·å–ç¶²å€ä¸¦è™•ç† &amp; è§£ç¢¼
def extract_urls_from_text(text):
    clean = html.unescape(html.unescape(text))
    urls = re.findall(r'https?://[^\s\]]+', clean)
    return list(dict.fromkeys(urls))

# â¤ ä¾ã€Œæ”¶æ–‡è™Ÿï¼šã€åˆ‡æ®µï¼ˆå¤šç­†ä¾†æ–‡ï¼‰
def split_blocks(text):
    parts = re.split(r"(?=æ”¶æ–‡è™Ÿï¼š\d+)", text)
    return [part.strip() for part in parts if part.strip()]

# â¤ èƒå–æ¨™é¡Œåç¨±ç”¨ä¾†å„²å­˜ Wordï¼ˆå¾ç¬¬ä¸€ç­†æ”¶æ–‡è™Ÿå»ºç«‹ï¼‰
def extract_filename(text_block):
    match = re.search(r"æ”¶æ–‡è™Ÿï¼š(\d+)", text_block)
    return f"æ”¶æ–‡è™Ÿ{match.group(1)}-æŸ¥æ ¸å ±å‘Š" if match else "æŸ¥æ ¸å ±å‘Š"

# âœ… è«‹å°‡å¤šç­†åŸå§‹ä¾†æ–‡è³‡æ–™è²¼å…¥ raw_text
raw_text = """
æ”¶æ–‡è™Ÿï¼š1140057499
ä¾†æ–‡æ—¥æœŸï¼š1141121
ä¾†æ–‡æ©Ÿé—œï¼šæ„è¦‹ä¿¡ç®±-æ¥Šå³°æº
ä¾†æ–‡è™Ÿï¼š2025112100008
å—æ–‡è€…ï¼šç¶“æ¿Ÿéƒ¨æ¨™æº–æª¢é©—å±€å°å—åˆ†å±€
é™„ä»¶æ•¸ï¼š3
é™„ä»¶æª”åï¼š20251121000081.jpgã€20251121000082.jpgã€20251121000083.jpg

ä¸»æ—¨ï¼šå¸‚å ´ç›£ç£ æ¥Šå³°æº (ç”³è«‹æµæ°´è™Ÿï¼š1141121000008) æœ‰è²¼å•†å“å®‰å…¨æ¨™ç« ï¼Œå“è³ªä¸ç¬¦è¦å®š,å­—è™Ÿæ¨™ç¤ºä¸ç¬¦ ç¶²è·¯å¹³å°

èªªæ˜ï¼š

[å…¶ä»–, JBL SOUNDGEAR FRAMES éŸ³æ¨‚å¤ªé™½çœ¼é¡, https://www.momoshop.com.tw/goods/GoodsDetail.jsp?i_code=13772321&amp;amp;Area=search&amp;amp;mdiv=403&amp;amp;oid=1_2&amp;amp;cid=index&amp;amp;kw=%E6%99%BA%E8%83%BD%E7%9C%BC%E9%8F%A1] [å…¶ä»–, SGæ™ºæ…§çœ¼é¡, https://www.momoshop.com.tw/goods/GoodsDetail.jsp?i_code=14033796&amp;amp;Area=search&amp;amp;mdiv=403&amp;amp;oid=1_3&amp;amp;cid=index&amp;amp;kw=%E6%99%BA%E8%83%BD%E7%9C%BC%E9%8F%A1] [å…¶ä»–, æ™ºèƒ½AIç¿»è­¯çœ¼é¡, https://www.momoshop.com.tw/TP/TP0004865/goodsDetail/TP00048650000033] [å…¶ä»–, æ™ºèƒ½AIç¿»è­¯çœ¼é¡ aiçœ¼é¡, https://www.momoshop.com.tw/TP/TP0004865/goodsDetail/TP00048650000032] [å…¶ä»–, å°ç£ç™¼è²¨ è¾¦å…¬ç¥å™¨ AIæ™ºèƒ½æ‹ç…§çœ¼é¡å…¨çƒç¿»è­¯, https://www.momoshop.com.tw/TP/TP0004865/goodsDetail/TP00048650000097]

æ­£æœ¬ï¼šç¶“æ¿Ÿéƒ¨æ¨™æº–æª¢é©—å±€
æ”¶æ–‡è™Ÿï¼š1140057500
ä¾†æ–‡æ—¥æœŸï¼š1141121
ä¾†æ–‡æ©Ÿé—œï¼šæ„è¦‹ä¿¡ç®±-æ¥Šå³°æº
ä¾†æ–‡è™Ÿï¼š2025112100009
å—æ–‡è€…ï¼šç¶“æ¿Ÿéƒ¨æ¨™æº–æª¢é©—å±€å°å—åˆ†å±€
é™„ä»¶æ•¸ï¼š3
é™„ä»¶æª”åï¼š20251121000091.jpgã€20251121000092.jpgã€20251121000093.jpg

ä¸»æ—¨ï¼šå¸‚å ´ç›£ç£ æ¥Šå³°æº (ç”³è«‹æµæ°´è™Ÿï¼š1141121000009) æœªè²¼å•†å“å®‰å…¨æ¨™ç« ,æœªæ¨™ç¤ºRå­—è»Œ ç¶²è·¯å¹³å°

èªªæ˜ï¼š

[å…¶ä»–, JBL SOUNDGEAR FRAMES éŸ³æ¨‚å¤ªé™½çœ¼é¡, https://www.momoshop.com.tw/goods/GoodsDetail.jsp?i_code=13772338&amp;amp;Area=search&amp;amp;mdiv=403&amp;amp;oid=1_6&amp;amp;cid=index&amp;amp;kw=%E6%99%BA%E8%83%BD%E7%9C%BC%E9%8F%A1] [å…¶ä»–, Xiaomi æ™ºæ…§éŸ³é »çœ¼é¡, https://www.momoshop.com.tw/goods/GoodsDetail.jsp?i_code=13586902&amp;amp;Area=search&amp;amp;mdiv=403&amp;amp;oid=1_1&amp;amp;cid=index&amp;amp;kw=%E6%99%BA%E8%83%BD%E7%9C%BC%E9%8F%A1] [å…¶ä»–, VIVE Eagle æ™ºæ…§çœ¼é¡, https://www.momoshop.com.tw/TP/TP0006668/goodsDetail/TP00066680000185]

æ­£æœ¬ï¼šç¶“æ¿Ÿéƒ¨æ¨™æº–æª¢é©—å±€



"""

# âœ… å–®ä¸€ Word è¼¸å‡ºï¼Œé€æ®µä¾†æ–‡ + æ­£ç¢ºæ’å…¥å•†å“å€å¡Šï¼ˆi å…¨åŸŸéå¢ & retry & delayï¼‰
doc = Document()
for sec in doc.sections:
    sec.top_margin = Cm(1.27)
    sec.bottom_margin = Cm(1.27)
    sec.left_margin = Cm(1.85)
    sec.right_margin = Cm(1.27)

blocks = split_blocks(raw_text)
global_index = 1  # ğŸ”º æ‰€æœ‰æ®µè½å…±ç”¨é€™å€‹å•†å“ç·¨è™Ÿè¨ˆæ•¸å™¨

for block in blocks:
    lines = block.strip().split("\n")
    urls = extract_urls_from_text(block)

    results = []
    for url in urls:
        r = parse_momo_simple(url)  # å·²å…§å»º retry & timeout
        r["ç·¨è™Ÿ"] = global_index
        global_index += 1
        results.append(r)
        time.sleep(5)

    for line in lines:
        doc.add_paragraph(line.strip())

        # åœ¨ã€Œæ­£æœ¬ï¼šã€è¡Œä¹‹å¾Œæ’å…¥è©²æ®µå°æ‡‰çš„å•†å“æŸ¥æ ¸çµæœ
        if line.strip().startswith("æ­£æœ¬ï¼š"):
            for row in results:
                i, name, prod, url = row["ç·¨è™Ÿ"], row["å•†å“åç¨±"], row["å“è™Ÿ"], row["ç¶²å€"]

                p1 = doc.add_paragraph()
                r1 = p1.add_run(f"{i}. {name}")
                r1.font.color.rgb = RGBColor(255, 0, 0)
                r1.font.size = Pt(12)

                p2 = doc.add_paragraph()
                r2 = p2.add_run("(æŸ¥ç„¡å•†å“æª¢é©—æ¨™è­˜)")
                r2.font.color.rgb = RGBColor(255, 0, 0)
                r2.font.size = Pt(12)

                doc.add_paragraph(f"å“è™Ÿ: {prod}")
                doc.add_paragraph(f"ç¶²å€: {url}")
                doc.add_paragraph()

# å„²å­˜ Wordï¼ˆä½¿ç”¨ç¬¬ä¸€ç­†æ®µçš„æ”¶æ–‡è™Ÿå‘½åï¼‰
fname = extract_filename(blocks[0]) + ".docx"
doc.save(fname)
print("âœ… å·²ç”¢å‡º Word æª”æ¡ˆï¼š", fname)